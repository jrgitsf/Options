# https://towardsdatascience.com/web-scraping-html-tables-with-python-c9baba21059
# https://stackabuse.com/converting-strings-to-datetime-in-python/
# https://kite.com/python/answers/how-to-remove-everything-after-a-character-in-a-string-in-python
# Exception Handling
# https://www.youtube.com/watch?v=NIWwJbo-9_8

from yahoo_fin.stock_info import *
import datetime as dt
from yahoo_fin.options import *
import requests
import lxml.html as lh
import pandas as pd
from functools import reduce
import pickle

# this ssl stuff is needed for pycharm to execute. Was not needed in jupyter
import ssl
ssl._create_default_https_context = ssl._create_unverified_context

expiration_threshold = 28
sp_greater_than_threshold=[]

#some test cases of the urls from the various websites
# print(build_url("cvx"))
# base_url = "https://query1.finance.yahoo.com/v8/finance/chart/"
# https://finance.yahoo.com/quote/CVX?p=CVX
base_url = "https://finance.yahoo.com/quote/" # this is required. Do NOT Delete

def build_jr_option_url(ticker, start_date=None, end_date=None, interval="1d"):
    if end_date is None:
        end_seconds = int(pd.Timestamp("now").timestamp())
    else:
        end_seconds = int(pd.Timestamp(end_date).timestamp())
    if start_date is None:
        start_seconds = 7223400
    else:
        start_seconds = int(pd.Timestamp(start_date).timestamp())

    # site = base_url + ticker + "?p=" + ticker # this is generated by yaho but could be shortened to the below
    site = base_url + ticker # this also works but I'm going with the yahoo return
    # "{}/v8/finance/chart/{}".format(self._base_url, self.ticker)
    # the cookbook had this but yahoo is not liking it
    # params = {"period1": start_seconds, "period2": end_seconds,
    #           "interval": interval.lower(), "events": "div,splits"}
    # return site, params

    return site

# This section is the essence of the scraping
# print(build_jr_option_url('cvx'))
# This works
url = build_jr_option_url('cvx')
# url='http://pokemondb.net/pokedex/all' # this was from the cookbook. I know it works. Good for a quick test
page = requests.get(url)
print(type(page))
doc = lh.fromstring(page.content)
print(type(doc))
print([e for e in doc])
tr_elements = doc.xpath('//tr')
print([T.text_content() for T in tr_elements])

# print([len(T) for T in tr_elements])

print(type(tr_elements))
#Create empty list
col=[]
i=0
#For each row, store each first element (header) and an empty list
for t in tr_elements:
    i+=1
    name=t.text_content()
    print('%d:"%s"'%(i,name), sep=' ')
    # print(i,name)
    col.append((name,[]))
print("------------------------------")

i=0
for name, value in tr_elements:
    i+=1
    print(name.text_content(), value.text_content())
    # print(i, "name", name.text_content())
    # print(i, "value", value.text_content())
    # name=t.text_content()
    # print('%d:"%s"'%(i,name), sep=' ')
    # print(i,name)
    # col.append((name,[]))

# row 12 (Starting from 0) is the earnings report date. Used this loop to help find that.
# This Works
# print('++++++++++++++++++++++')
# for name, value in tr_elements[12:13]:
#     # print(name.text_content(), value.text_content())
#     print("name", name.text_content())
#     print("value", value.text_content())

# This is more streamlined to just get the earnings date
# This Works
# print('>>>>>>>>>>>>>')
# name, value = tr_elements[12]
# print(url)
# print("name", name.text_content())
# print("value", value.text_content())

sp = tickers_sp500()
print(sp)
print(type(sp))

# for ticker in sp:
#     ticker_str = ticker.replace(".", '-')
#     # print(ticker_str)
#     url = build_jr_option_url(ticker_str)
#     # print(url)
#     page = requests.get(url)
#     doc = lh.fromstring(page.content)
#     tr_elements = doc.xpath('//tr')
#     for name, value in tr_elements[12:13]:
#         # print(name.text_content(), value.text_content())
#         # print("name", name.text_content())
        # print("value", value.text_content())

j=0
counter_greater_than_threshold = 0
counter_less_than_threshold = 0
counter_N_A = 0
counter_bad_poll = 0
# for ticker in sp:
for ticker in sp[:200]:
    print(j)
    ticker_str = ticker.replace(".", '-')
    # print(ticker_str)
    url = build_jr_option_url(ticker_str)
    print(url)
    page = requests.get(url) # returns html response code 200 is successful
    print(page)
    doc = lh.fromstring(page.content)
    # print([d.text_content() for d in doc]) # used for troubleshooting. Produces tons of text!!!
    tr_elements = doc.xpath('//tr')
    print([T.text_content() for T in tr_elements]) # used for troubleshooting
    # name, value, *other = tr_elements[12]  # row 12 is the earnings date. The other was due to some dates coming out of range
    try:
        name, value, *other = tr_elements[12] # row 12 is the earnings date. The other was due to some dates coming out of range
    except IndexError as e:
        print('Index error-->', e)
        # pass
    except Exception as e:
        print(e)
        # pass
    print(type(name), type(value), type(other))
    print("name", name.text_content()) # this converts the lxml to the content instead of the pointer.
    print("value", value.text_content())
    string_value = str(value.text_content()) # this converts to a string
    print("string value", string_value)
    j +=1 # a counter so we can follow how far we got in the list

    earnings_date_split = string_value.split(" -", 1) # this splits the earning date if it gives ranges
    print(type(earnings_date_split))
    substring = earnings_date_split[0] # [0] displays just the first part of the split. If you do not use it still works
    print(substring)

    # b is for the short month naming convention
    # if earnings_date_split[0] != 'N/A':
    #     date_time_earnings_date_split = dt.datetime.strptime(earnings_date_split[0], '%b %d, %Y')
    #     print(date_time_earnings_date_split)

# b is for the short month naming convention
    if earnings_date_split[0] != 'N/A':
        try:
            date_time_earnings_date_split = dt.datetime.strptime(earnings_date_split[0], '%b %d, %Y')
            print(date_time_earnings_date_split)
        except ValueError as e:
            print("Value Error -->",e)
            counter_bad_poll +=1
        except Exception as e:
            print(e)
            # pass
    else:
        counter_N_A +=1

    print("delta time",date_time_earnings_date_split-dt.datetime.now())
    print(dt.timedelta(days=expiration_threshold))
    if date_time_earnings_date_split-dt.datetime.now() >= dt.timedelta(days=expiration_threshold):
        print(ticker, " greater than expiration_threshold ",date_time_earnings_date_split-dt.datetime.now())
        sp_greater_than_threshold.append(ticker)
        counter_greater_than_threshold +=1
    else:
        print(ticker, " less than expiration_threshold")
        counter_less_than_threshold +=1

print(sp_greater_than_threshold)
print("greater than = ", counter_greater_than_threshold)
print('less than = ',counter_less_than_threshold)
print('Number of N/As = ',counter_N_A)
print('Number of bad polls = ', counter_bad_poll)

price_data = {ticker: get_data(ticker.replace(".", "-"), start_date="06/01/2020") for ticker in sp_greater_than_threshold}
# print(price_data)
combined = reduce(lambda x, y: x.append(y), price_data.values())
print(combined)
print(combined.to_string())

print("oooooooooooooooooooooooooooooooooooo")
options_data = {ticker: get_calls(ticker.replace(".", "-")) for ticker in sp_greater_than_threshold}
# print(price_data)
combined_options = reduce(lambda x, y: x.append(y), options_data.values())
print(combined_options)
print(combined_options.to_string())

with open("sp500tickers_greater_threshold.pickle_all5000", "wb") as f:
    pickle.dump(combined_options, f)